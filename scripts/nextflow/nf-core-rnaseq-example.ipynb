{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biotech Blueprint Example Nextflow Notebook\n",
    "\n",
    "At a high level, this notebook does the following:\n",
    "\n",
    "1. Copies the example `nf-core/rnaseq/` workflow from THIS Jupyter environment to S3 (`workflowBucket`/`workflowFolderPrefix`).\n",
    "2. Submits the Nextflow head node which downloads the `nf-core/rnaseq/` workflow from that S3 location.\n",
    "3. As the Nextflow head node runs, any subsequent nextflow processes will be submitted as addtional jobs.\n",
    "4. Tail the head node's CloudWatch Logs output to observe progress.\n",
    "\n",
    "![alt text](nextflowbatchhelper/bb.nextflow.diagram.png \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initalize the APIs, worfkflow staging location, job queue/definition. If you deployed this using the Biotech Blueprint Informatics Catalog, these values will have already been set for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import time\n",
    "batchClient = boto3.client('batch')\n",
    "s3Client = boto3.resource('s3')\n",
    "logClient = boto3.client('logs')\n",
    "\n",
    "workflowBucket = 'xxWorkflowBucketxx' # replace with your bucket name\n",
    "workflowFolderPrefix = 'xxWorkflowPrefixxx' # replace with your object key\n",
    "jobQueueName = 'xxJobQueuexx'\n",
    "headNodeJobDef = 'nextflow'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage the workflow folder from this Juptyer enviornment to the S3 bucket the head node will download it from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash -s \"$workflowBucket\" \"$workflowFolderPrefix\"\n",
    "aws s3 sync --only-show-errors --exclude '.*' nf-core/rnaseq s3://$1/$2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are submitting the head node to batch. There are a few key points to highlight:\n",
    "\n",
    "The Nextflow container is setup such that the first parameter is the staging location of the workflow folder. Any addtional parameters are passed, in order, directly to the Nextflow command entrypoint. \n",
    "\n",
    "* Note that the `--reads` parameter utilizes Nextflows glob matching pattern. If there were more than one pair in that 1000 Genomes s3 location, it would kick off the rnaseq pipeline for every pair it matched. Just one job submission can kick of 1000s of jobs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = batchClient.submit_job(\n",
    "    jobDefinition=headNodeJobDef,\n",
    "    jobName='rnaseq-NextflowHeadNode',\n",
    "    jobQueue=jobQueueName,\n",
    "    containerOverrides={\n",
    "        'command': [\n",
    "            \"s3://{0}/{1}\".format(workflowBucket, workflowFolderPrefix),\n",
    "            \"--reads\", \"s3://1000genomes/phase3/data/HG00243/sequence_read/SRR*_{1,2}.filt.fastq.gz\",\n",
    "            \"--genome\", \"GRCh37\",\n",
    "            \"--skip_qc\"\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "headNodeJobId = response['jobId']\n",
    "print('Job ID: {0}'.format(headNodeJobId))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hastily written method to tail the head node's CloudWatch logs to follow the progress of the job... You can also observe progress by looking in the AWS Batch Console. You can go even deeper by looking at the CloudWatch log output of any process Nextflow has started.\n",
    "* Note: If this is the first time you are running something or Batch there will be a slight delay (10s of seconds) while compute resources are provisioned. You will experience a similar delay if Batch ever scales down your desired CPU count to zero because there are no jobs to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nextflowbatchhelper.tailCloudwatch import cloudWatchTail\n",
    "tail = cloudWatchTail()\n",
    "tail.startTail(headNodeJobId)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
